{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T07:09:51.131515Z",
     "start_time": "2025-05-02T07:09:49.177508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# edgevit.py\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from timm.models.vision_transformer import _cfg\n",
    "from timm.models import register_model\n",
    "from timm.models.layers import trunc_normal_, DropPath, to_2tuple\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CMlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Conv2d(in_features, hidden_features, 1)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Conv2d(hidden_features, out_features, 1)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, dim, reduction=4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim // reduction)  # Reduce feature size\n",
    "        self.fc2 = nn.Linear(dim // reduction, dim)  # Restore original size\n",
    "        self.act = nn.ReLU()  # Non-linearity\n",
    "        self.sigmoid = nn.Sigmoid()  # Convert values to attention weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input: x of shape [B, C, H, W]  (Batch, Channels, Height, Width)\n",
    "        Output: x of shape [B, C, H, W] (Reweighted Feature Map)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Step 1: Squeeze - Global Average Pooling (GAP)\n",
    "        w = x.mean(dim=[2, 3])  # Shape: [B, C]\n",
    "\n",
    "        # Step 2: Excite - Learn importance of channels\n",
    "        w = self.fc1(w)  # Reduce feature dimension\n",
    "        w = self.act(w)  # Apply ReLU\n",
    "        w = self.fc2(w)  # Restore feature dimension\n",
    "        w = self.sigmoid(w)  # Normalize values to [0,1]\n",
    "\n",
    "        # Step 3: Scale - Multiply attention weights with original feature map\n",
    "        w = w.unsqueeze(-1).unsqueeze(-1)  # Reshape to [B, C, 1, 1]\n",
    "        return x * w  # Element-wise multiplication (channel-wise attention)\n",
    "\n",
    "\n",
    "class GlobalSparseAttn(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        # self.upsample = nn.Upsample(scale_factor=sr_ratio, mode='nearest')\n",
    "        self.sr = sr_ratio\n",
    "        if self.sr > 1:\n",
    "            self.sampler = nn.AvgPool2d(1, sr_ratio)\n",
    "            kernel_size = sr_ratio\n",
    "            self.LocalProp = nn.ConvTranspose2d(dim, dim, kernel_size, stride=sr_ratio, groups=dim)\n",
    "            self.norm = nn.LayerNorm(dim)\n",
    "        else:\n",
    "            self.sampler = nn.Identity()\n",
    "            self.upsample = nn.Identity()\n",
    "            self.norm = nn.Identity()\n",
    "\n",
    "    def forward(self, x, H: int, W: int):\n",
    "        B, N, C = x.shape\n",
    "        if self.sr > 1.:\n",
    "            x = x.transpose(1, 2).reshape(B, C, H, W)\n",
    "            x = self.sampler(x)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, -1, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, -1, C)\n",
    "\n",
    "        if self.sr > 1:\n",
    "            x = x.permute(0, 2, 1).reshape(B, C, int(H / self.sr), int(W / self.sr))\n",
    "            x = self.LocalProp(x)\n",
    "            x = x.reshape(B, C, -1).permute(0, 2, 1)\n",
    "            x = self.norm(x)\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# class LocalAgg(nn.Module):\n",
    "#     def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "#                  drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "#         super().__init__()\n",
    "#         self.pos_embed = nn.Conv2d(dim, dim, 3, padding=1, groups=dim)\n",
    "#         self.norm1 = nn.BatchNorm2d(dim)\n",
    "#         self.conv1 = nn.Conv2d(dim, dim, 1)\n",
    "#         self.conv2 = nn.Conv2d(dim, dim, 1)\n",
    "#         self.attn = nn.Conv2d(dim, dim, 5, padding=2, groups=dim)\n",
    "#         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "#         self.norm2 = nn.BatchNorm2d(dim)\n",
    "#         mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "#         self.mlp = CMlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.pos_embed(x)\n",
    "#         x = x + self.drop_path(self.conv2(self.attn(self.conv1(self.norm1(x)))))\n",
    "#         x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "#         return x\n",
    "\n",
    "class LocalAgg(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.pos_embed = nn.Conv2d(dim, dim, 3, padding=1, groups=dim)  # Positional encoding\n",
    "        self.attn = nn.Conv2d(dim, dim, 5, padding=2, groups=dim)  # Local attention\n",
    "        self.norm1 = nn.BatchNorm2d(dim)\n",
    "        self.norm2 = nn.BatchNorm2d(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = CMlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.se = SEBlock(dim) \n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        # Step 1: Local Feature Extraction\n",
    "        x = x + self.pos_embed(x)\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "\n",
    "        # Step 2: Apply SE Attention (Enhance Important Channels)\n",
    "        x = self.se(x)\n",
    "\n",
    "        # Step 3: MLP Block\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x + identity  # Residual connection\n",
    "\n",
    "\n",
    "\n",
    "class SelfAttn(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1.):\n",
    "        super().__init__()\n",
    "        self.pos_embed = nn.Conv2d(dim, dim, 3, padding=1, groups=dim)\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = GlobalSparseAttn(\n",
    "            dim,\n",
    "            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        # global layer_scale\n",
    "        # self.ls = layer_scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_embed(x)\n",
    "        B, N, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        x = x.transpose(1, 2).reshape(B, N, H, W)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LGLBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1.):\n",
    "        super().__init__()\n",
    "\n",
    "        if sr_ratio > 1:\n",
    "            self.LocalAgg = LocalAgg(dim, num_heads, mlp_ratio, qkv_bias, qk_scale, drop, attn_drop, drop_path,\n",
    "                                     act_layer, norm_layer)\n",
    "        else:\n",
    "            self.LocalAgg = nn.Identity()\n",
    "\n",
    "        self.SelfAttn = SelfAttn(dim, num_heads, mlp_ratio, qkv_bias, qk_scale, drop, attn_drop, drop_path, act_layer,\n",
    "                                 norm_layer, sr_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.LocalAgg(x)\n",
    "        x = self.SelfAttn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        return x\n",
    "\n",
    "\n",
    "class EdgeVit(nn.Module):\n",
    "    \"\"\" Vision Transformer\n",
    "    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`  -\n",
    "        https://arxiv.org/abs/2010.11929\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, depth=[1, 2, 5, 3], img_size=224, in_chans=3, num_classes=1000, embed_dim=[48, 96, 240, 384],\n",
    "                 head_dim=64, mlp_ratio=4., qkv_bias=True, qk_scale=None, representation_size=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=None, sr_ratios=[4, 2, 2, 1], **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            depth (list): depth of each stage\n",
    "            img_size (int, tuple): input image size\n",
    "            in_chans (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (list): embedding dimension of each stage\n",
    "            head_dim (int): head dimension\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            drop_rate (float): dropout rate\n",
    "            attn_drop_rate (float): attention dropout rate\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            norm_layer (nn.Module): normalization layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "\n",
    "        self.patch_embed1 = PatchEmbed(\n",
    "            img_size=img_size, patch_size=4, in_chans=in_chans, embed_dim=embed_dim[0])\n",
    "        self.patch_embed2 = PatchEmbed(\n",
    "            img_size=img_size // 4, patch_size=2, in_chans=embed_dim[0], embed_dim=embed_dim[1])\n",
    "        self.patch_embed3 = PatchEmbed(\n",
    "            img_size=img_size // 8, patch_size=2, in_chans=embed_dim[1], embed_dim=embed_dim[2])\n",
    "        self.patch_embed4 = PatchEmbed(\n",
    "            img_size=img_size // 16, patch_size=2, in_chans=embed_dim[2], embed_dim=embed_dim[3])\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depth))]  # stochastic depth decay rule\n",
    "        num_heads = [dim // head_dim for dim in embed_dim]\n",
    "        self.blocks1 = nn.ModuleList([\n",
    "            LGLBlock(\n",
    "                dim=embed_dim[0], num_heads=num_heads[0], mlp_ratio=mlp_ratio[0], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                sr_ratio=sr_ratios[0])\n",
    "            for i in range(depth[0])])\n",
    "        self.blocks2 = nn.ModuleList([\n",
    "            LGLBlock(\n",
    "                dim=embed_dim[1], num_heads=num_heads[1], mlp_ratio=mlp_ratio[1], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i + depth[0]], norm_layer=norm_layer,\n",
    "                sr_ratio=sr_ratios[1])\n",
    "            for i in range(depth[1])])\n",
    "        self.blocks3 = nn.ModuleList([\n",
    "            LGLBlock(\n",
    "                dim=embed_dim[2], num_heads=num_heads[2], mlp_ratio=mlp_ratio[2], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i + depth[0] + depth[1]], norm_layer=norm_layer,\n",
    "                sr_ratio=sr_ratios[2])\n",
    "            for i in range(depth[2])])\n",
    "        self.blocks4 = nn.ModuleList([\n",
    "            LGLBlock(\n",
    "                dim=embed_dim[3], num_heads=num_heads[3], mlp_ratio=mlp_ratio[3], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i + depth[0] + depth[1] + depth[2]],\n",
    "                norm_layer=norm_layer, sr_ratio=sr_ratios[3])\n",
    "            for i in range(depth[3])])\n",
    "        self.norm = nn.BatchNorm2d(embed_dim[-1])\n",
    "\n",
    "        # Representation layer\n",
    "        if representation_size:\n",
    "            self.num_features = representation_size\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                ('fc', nn.Linear(embed_dim, representation_size)),\n",
    "                ('act', nn.Tanh())\n",
    "            ]))\n",
    "        else:\n",
    "            self.pre_logits = nn.Identity()\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(embed_dim[-1], num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed1(x)\n",
    "        x = self.pos_drop(x)\n",
    "        for blk in self.blocks1:\n",
    "            x = blk(x)\n",
    "        x = self.patch_embed2(x)\n",
    "        for blk in self.blocks2:\n",
    "            x = blk(x)\n",
    "        x = self.patch_embed3(x)\n",
    "        for blk in self.blocks3:\n",
    "            x = blk(x)\n",
    "        x = self.patch_embed4(x)\n",
    "        for blk in self.blocks4:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pre_logits(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = x.flatten(2).mean(-1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "@register_model\n",
    "def edgevit_xxs(pretrained=True, **kwargs):\n",
    "    model = EdgeVit(\n",
    "        depth=[1, 1, 3, 2],\n",
    "        embed_dim=[36, 72, 144, 288], head_dim=36, mlp_ratio=[4] * 4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), sr_ratios=[4, 2, 2, 1], **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def edgevit_xs(pretrained=True, **kwargs):\n",
    "    model = EdgeVit(\n",
    "        depth=[1, 1, 3, 1],\n",
    "        embed_dim=[48, 96, 240, 384], head_dim=48, mlp_ratio=[4] * 4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), sr_ratios=[4, 2, 2, 1], **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def edgevit_s(pretrained=True, **kwargs):\n",
    "    model = EdgeVit(\n",
    "        depth=[1, 2, 5, 3],\n",
    "        embed_dim=[48, 96, 240, 384], head_dim=48, mlp_ratio=[4] * 4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), sr_ratios=[4, 2, 2, 1], **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    return model\n",
    "    \n",
    "@register_model\n",
    "def edgevit_m(pretrained=False, **kwargs):\n",
    "    model = EdgeVit(\n",
    "        depth=[2, 3, 6, 4],  # Increased depth for more layers per stage\n",
    "        embed_dim=[64, 128, 320, 512],  # Wider embeddings for higher capacity\n",
    "        head_dim=64,  # Increased head dimension for finer attention\n",
    "        mlp_ratio=[4, 4, 4.5, 4.5],  # Slightly higher MLP ratio in later stages\n",
    "        qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        sr_ratios=[4, 2, 2, 1],  # Kept the same, as these are tied to spatial reduction\n",
    "        **kwargs\n",
    "    )\n",
    "    model.default_cfg = _cfg()\n",
    "    return model\n"
   ],
   "id": "5fbf4de61982b0c1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhishek/PycharmProjects/EdgeVit_FaceEmo_Faster/.venv/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T19:18:32.194856Z",
     "start_time": "2025-05-02T19:18:28.795872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Real-time Emotion Recognition Webcam Application using EdgeViT_XXS\n",
    "\n",
    "This script:\n",
    "1. Captures video from your webcam\n",
    "2. Detects faces using MediaPipe\n",
    "3. Preprocesses faces with frequency transforms\n",
    "4. Performs emotion classification using a pretrained EdgeViT_XXS model\n",
    "5. Displays results in real-time with visualization\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import mediapipe as mp\n",
    "\n",
    "# Emotion labels mapping \n",
    "EMOTION_MAP = {\n",
    "    \"angry\": 2, \"disgust\": 1, \"fear\": 4, \"happy\": 6,\n",
    "    \"neutral\": 3, \"sad\": 0, \"surprise\": 5\n",
    "}\n",
    "\n",
    "EMOTION_LABELS = {v: k for k, v in EMOTION_MAP.items()}\n",
    "\n",
    "class EmotionDetector:\n",
    "    def __init__(self, model_path):\n",
    "        \"\"\"Initialize emotion detector with pretrained EdgeViT_XXS model.\"\"\"\n",
    "        # Check for GPU\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        try:\n",
    "            self.model = edgevit_xxs(num_classes=7)  \n",
    "            state_dict = torch.load(model_path, map_location=self.device)\n",
    "            self.model.load_state_dict(state_dict)  # Load weights\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()  \n",
    "            print(f\"Successfully loaded model from {model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "        \n",
    "        # MediaPipe\n",
    "        self.mp_face_detection = mp.solutions.face_detection\n",
    "        self.face_detection = self.mp_face_detection.FaceDetection(\n",
    "            model_selection=1,  # 1 for full-range detection \n",
    "            min_detection_confidence=0.5\n",
    "        )\n",
    "        \n",
    "      \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "    \n",
    "    def frequency_transform(self, image):\n",
    "        \"\"\"Apply frequency transform to create 3-channel representation.\"\"\"\n",
    "        # Convert to grayscale if needed\n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:  \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Normalize pixel values\n",
    "        image = image.astype(np.float32) / 255.0  \n",
    "\n",
    "        # Low-Pass Filter (Gaussian Blur)\n",
    "        low_pass = cv2.GaussianBlur(image, (7, 7), 1)\n",
    "\n",
    "        # High-Pass Filter (Sobel Edge Detection)\n",
    "        sobelx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        sobely = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "        high_pass = cv2.magnitude(sobelx, sobely)\n",
    "        high_pass = high_pass / np.max(high_pass) if np.max(high_pass) > 0 else high_pass  # Normalize\n",
    "\n",
    "        # Stack into 3-channel image (Original, Low-Pass, High-Pass)\n",
    "        freq_image = np.stack([image, low_pass, high_pass], axis=-1)\n",
    "        return freq_image\n",
    "    \n",
    "    def detect_face(self, frame):\n",
    "        \"\"\"Detect face in the input frame.\"\"\"\n",
    "        \n",
    "        # Convert to RGB for MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process frame with face detection\n",
    "        results = self.face_detection.process(frame_rgb)\n",
    "        \n",
    "        # Check if any faces detected\n",
    "        if not results.detections:\n",
    "            return None, None\n",
    "        \n",
    "        # Get the bounding box of the first detected face\n",
    "        detection = results.detections[0]  # Using only singl first face\n",
    "        bboxC = detection.location_data.relative_bounding_box\n",
    "        \n",
    "    \n",
    "        h, w, _ = frame.shape\n",
    "        \n",
    "       \n",
    "        x = max(0, int(bboxC.xmin * w))\n",
    "        y = max(0, int(bboxC.ymin * h))\n",
    "        width = min(int(bboxC.width * w), w - x)\n",
    "        height = min(int(bboxC.height * h), h - y)\n",
    "        \n",
    "        # Extract face region\n",
    "        face_region = frame[y:y+height, x:x+width]\n",
    "        \n",
    "        # Return face region and bounding box\n",
    "        return face_region, (x, y, width, height)\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"Preprocess image for model inference.\"\"\"\n",
    "        # Apply frequency transform\n",
    "        freq_image = self.frequency_transform(image)\n",
    "        \n",
    "\n",
    "        freq_image = (freq_image * 255).astype(np.uint8)\n",
    "        pil_image = Image.fromarray(freq_image)\n",
    "        \n",
    "        # Apply transforms\n",
    "        tensor_image = self.transform(pil_image)\n",
    "        \n",
    "        # Add batch dimension\n",
    "        tensor_image = tensor_image.unsqueeze(0).to(self.device)\n",
    "        return tensor_image\n",
    "    \n",
    "    def predict_emotion(self, face_image):\n",
    "        \"\"\"Predict emotion from face image.\"\"\"\n",
    "        # Check if face image is valid\n",
    "        if face_image is None or face_image.size == 0:\n",
    "            return None, None\n",
    "        \n",
    "        # Preprocess and predict\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                tensor_image = self.preprocess_image(face_image)\n",
    "                outputs = self.model(tensor_image)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                # Get probabilities\n",
    "                probabilities = torch.nn.functional.softmax(outputs, dim=1)[0]\n",
    "                \n",
    "                emotion_idx = predicted.item()\n",
    "                confidence = probabilities[emotion_idx].item()\n",
    "                emotion_name = EMOTION_LABELS[emotion_idx]\n",
    "                \n",
    "                return emotion_name, probabilities.cpu().numpy()\n",
    "            except Exception as e:\n",
    "                print(f\"Error during prediction: {e}\")\n",
    "                return None, None\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def run_webcam_app():\n",
    "    \"\"\"Run the webcam application for real-time emotion detection.\"\"\"\n",
    "  \n",
    "    model_path = '/Users/abhishek/Downloads/edgevit_finetuned.pth'\n",
    "    \n",
    "    # Initialize emotion detector\n",
    "    detector = EmotionDetector(model_path)\n",
    "    \n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)  # 0 is usually the default webcam\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "    \n",
    "    # Setting webcam resolution \n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    \n",
    "    # Variables for FPS calculation\n",
    "    fps_counter = 0\n",
    "    fps_start_time = time.time()\n",
    "    fps = 0\n",
    "    \n",
    "    print(\"Starting webcam emotion detection. Press 'q' to quit.\")\n",
    "    \n",
    "    # UI settings\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    line_thickness = 2\n",
    "    \n",
    "    while True:\n",
    "        # Read frame from webcam\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture image\")\n",
    "            break\n",
    "        \n",
    "        # Create a copy of the frame for display\n",
    "        display_frame = frame.copy()\n",
    "        \n",
    "        # Update FPS calculation\n",
    "        fps_counter += 1\n",
    "        if fps_counter >= 10:  # Calculate FPS every 10 frames\n",
    "            current_time = time.time()\n",
    "            fps = fps_counter / (current_time - fps_start_time)\n",
    "            fps_start_time = current_time\n",
    "            fps_counter = 0\n",
    "        \n",
    "        # Detect face\n",
    "        face_image, face_bbox = detector.detect_face(frame)\n",
    "        \n",
    "        # Draw face detection result\n",
    "        if face_image is not None and face_bbox is not None:\n",
    "            # Draw bounding box around face\n",
    "            x, y, w, h = face_bbox\n",
    "            cv2.rectangle(display_frame, (x, y), (x+w, y+h), (0, 255, 0), line_thickness)\n",
    "            \n",
    "            # Predict emotion\n",
    "            emotion, probabilities = detector.predict_emotion(face_image)\n",
    "            \n",
    "            if emotion and probabilities is not None:\n",
    "                # Display emotion above face\n",
    "                text_y_position = max(y - 10, 20)  # Ensure text doesn't go off-screen\n",
    "                cv2.putText(display_frame, \n",
    "                            f\"{emotion.upper()}\",\n",
    "                            (x, text_y_position),\n",
    "                            font, 0.8, (0, 255, 0), line_thickness)\n",
    "                \n",
    "                # Draw emotion probability bars\n",
    "                bar_width = 150\n",
    "                bar_height = 15\n",
    "                bar_gap = 5\n",
    "                bar_x = 10\n",
    "                bar_y = 40\n",
    "                \n",
    "                # Display emotion probabilities\n",
    "                \n",
    "                for i, emotion_name in enumerate(EMOTION_LABELS.values()):\n",
    "                    # Emotion index\n",
    "                    emotion_idx = list(EMOTION_LABELS.keys())[i]\n",
    "                    prob = probabilities[emotion_idx]\n",
    "                    \n",
    "                    # Emotion label\n",
    "                    cv2.putText(display_frame, emotion_name, \n",
    "                                (bar_x, bar_y + i*(bar_height+bar_gap+10)), \n",
    "                                font, 0.5, (255, 255, 255), 1)\n",
    "                    \n",
    "                    # Probability bar background (gray)\n",
    "                    cv2.rectangle(display_frame, \n",
    "                                 (bar_x + 80, bar_y + i*(bar_height+bar_gap+10) - bar_height),\n",
    "                                 (bar_x + 80 + bar_width, bar_y + i*(bar_height+bar_gap+10)),\n",
    "                                 (100, 100, 100), -1)\n",
    "                    \n",
    "                    # Probability bar (green)\n",
    "                    bar_length = int(prob * bar_width)\n",
    "                    cv2.rectangle(display_frame, \n",
    "                                 (bar_x + 80, bar_y + i*(bar_height+bar_gap+10) - bar_height),\n",
    "                                 (bar_x + 80 + bar_length, bar_y + i*(bar_height+bar_gap+10)),\n",
    "                                 (0, 255, 0), -1)\n",
    "                    \n",
    "                    # Display probability percentage\n",
    "                    cv2.putText(display_frame, f\"{prob*100:.1f}%\", \n",
    "                               (bar_x + 80 + bar_width + 5, bar_y + i*(bar_height+bar_gap+10)),\n",
    "                               font, 0.5, (255, 255, 255), 1)\n",
    "                \n",
    "                # Show the frequency transform of the face\n",
    "                freq_face = detector.frequency_transform(face_image)\n",
    "                # Scale to 0-255 for display\n",
    "                freq_face_display = (freq_face * 255).astype(np.uint8)\n",
    "                # Resize for better display\n",
    "                freq_face_display = cv2.resize(freq_face_display, (224, 224))\n",
    "                cv2.imshow(\"Frequency Transform\", freq_face_display)\n",
    "        else:\n",
    "            # No face detected\n",
    "            cv2.putText(display_frame, \"No face detected\", (10, 30), \n",
    "                        font, 0.8, (0, 0, 255), line_thickness)\n",
    "        \n",
    "        # Display FPS\n",
    "        cv2.putText(display_frame, f\"FPS: {int(fps)}\", \n",
    "                   (display_frame.shape[1]-120, 30), \n",
    "                   font, 0.7, (0, 0, 255), 2)\n",
    "        \n",
    "        # Display the result\n",
    "        cv2.imshow(\"Emotion Recognition\", display_frame)\n",
    "        \n",
    "        # Check for key press to exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Application closed.\")\n"
   ],
   "id": "9247cdcd9a8ee809"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "run_webcam_app()",
   "id": "33467456625ed83f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
