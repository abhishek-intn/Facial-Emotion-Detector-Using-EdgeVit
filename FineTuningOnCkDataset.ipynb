{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1351797,"sourceType":"datasetVersion","datasetId":786787},{"sourceId":4957114,"sourceType":"datasetVersion","datasetId":2820176},{"sourceId":4959552,"sourceType":"datasetVersion","datasetId":2876250}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install --upgrade scikit-learn imbalanced-learn\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# edgevit.py\n\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nfrom functools import partial\nimport torch.nn.functional as F\nimport math\nfrom timm.models.vision_transformer import _cfg\nfrom timm.models import register_model\nfrom timm.models.layers import trunc_normal_, DropPath, to_2tuple\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass CMlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Conv2d(in_features, hidden_features, 1)\n        self.act = act_layer()\n        self.fc2 = nn.Conv2d(hidden_features, out_features, 1)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nimport torch\nimport torch.nn as nn\n\nclass SEBlock(nn.Module):\n    def __init__(self, dim, reduction=4):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim // reduction)  \n        self.fc2 = nn.Linear(dim // reduction, dim)  \n        self.act = nn.ReLU() \n        self.sigmoid = nn.Sigmoid() \n\n    def forward(self, x):\n        \"\"\"\n        Input: x of shape [B, C, H, W]  (Batch, Channels, Height, Width)\n        Output: x of shape [B, C, H, W] (Reweighted Feature Map)\n        \"\"\"\n        B, C, H, W = x.shape\n\n        # Squeeze - Global Average Pooling (GAP)\n        w = x.mean(dim=[2, 3])  # Shape: [B, C]\n\n        # Excite - Learn importance of channels\n        w = self.fc1(w) \n        w = self.act(w)  \n        w = self.fc2(w)  \n        w = self.sigmoid(w) \n\n        #Multiply attention weights with original feature map\n        w = w.unsqueeze(-1).unsqueeze(-1) \n        return x * w  \n\n\nclass GlobalSparseAttn(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        # self.upsample = nn.Upsample(scale_factor=sr_ratio, mode='nearest')\n        self.sr = sr_ratio\n        if self.sr > 1:\n            self.sampler = nn.AvgPool2d(1, sr_ratio)\n            kernel_size = sr_ratio\n            self.LocalProp = nn.ConvTranspose2d(dim, dim, kernel_size, stride=sr_ratio, groups=dim)\n            self.norm = nn.LayerNorm(dim)\n        else:\n            self.sampler = nn.Identity()\n            self.upsample = nn.Identity()\n            self.norm = nn.Identity()\n\n    def forward(self, x, H: int, W: int):\n        B, N, C = x.shape\n        if self.sr > 1.:\n            x = x.transpose(1, 2).reshape(B, C, H, W)\n            x = self.sampler(x)\n            x = x.flatten(2).transpose(1, 2)\n\n        qkv = self.qkv(x).reshape(B, -1, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, -1, C)\n\n        if self.sr > 1:\n            x = x.permute(0, 2, 1).reshape(B, C, int(H / self.sr), int(W / self.sr))\n            x = self.LocalProp(x)\n            x = x.reshape(B, C, -1).permute(0, 2, 1)\n            x = self.norm(x)\n\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\n# class LocalAgg(nn.Module):\n#     def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n#                  drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n#         super().__init__()\n#         self.pos_embed = nn.Conv2d(dim, dim, 3, padding=1, groups=dim)\n#         self.norm1 = nn.BatchNorm2d(dim)\n#         self.conv1 = nn.Conv2d(dim, dim, 1)\n#         self.conv2 = nn.Conv2d(dim, dim, 1)\n#         self.attn = nn.Conv2d(dim, dim, 5, padding=2, groups=dim)\n#         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n#         self.norm2 = nn.BatchNorm2d(dim)\n#         mlp_hidden_dim = int(dim * mlp_ratio)\n#         self.mlp = CMlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n#\n#     def forward(self, x):\n#         x = x + self.pos_embed(x)\n#         x = x + self.drop_path(self.conv2(self.attn(self.conv1(self.norm1(x)))))\n#         x = x + self.drop_path(self.mlp(self.norm2(x)))\n#         return x\n\nclass LocalAgg(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.pos_embed = nn.Conv2d(dim, dim, 3, padding=1, groups=dim)  # Positional encoding\n        self.attn = nn.Conv2d(dim, dim, 5, padding=2, groups=dim)  # Local attention\n        self.norm1 = nn.BatchNorm2d(dim)\n        self.norm2 = nn.BatchNorm2d(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = CMlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.se = SEBlock(dim) \n\n    def forward(self, x):\n        identity = x\n\n       \n        x = x + self.pos_embed(x)\n        x = x + self.attn(self.norm1(x))\n\n     \n        x = self.se(x)\n\n       \n        x = x + self.mlp(self.norm2(x))\n        return x + identity  # Residual connection\n\n\n\nclass SelfAttn(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1.):\n        super().__init__()\n        self.pos_embed = nn.Conv2d(dim, dim, 3, padding=1, groups=dim)\n        self.norm1 = norm_layer(dim)\n        self.attn = GlobalSparseAttn(\n            dim,\n            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n        # global layer_scale\n        # self.ls = layer_scale\n\n    def forward(self, x):\n        x = x + self.pos_embed(x)\n        B, N, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)\n        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        x = x.transpose(1, 2).reshape(B, N, H, W)\n        return x\n\n\nclass LGLBlock(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1.):\n        super().__init__()\n\n        if sr_ratio > 1:\n            self.LocalAgg = LocalAgg(dim, num_heads, mlp_ratio, qkv_bias, qk_scale, drop, attn_drop, drop_path,\n                                     act_layer, norm_layer)\n        else:\n            self.LocalAgg = nn.Identity()\n\n        self.SelfAttn = SelfAttn(dim, num_heads, mlp_ratio, qkv_bias, qk_scale, drop, attn_drop, drop_path, act_layer,\n                                 norm_layer, sr_ratio)\n\n    def forward(self, x):\n        x = self.LocalAgg(x)\n        x = self.SelfAttn(x)\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n        self.norm = nn.LayerNorm(embed_dim)\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x)\n        B, C, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)\n        x = self.norm(x)\n        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        return x\n\n\nclass EdgeVit(nn.Module):\n    \"\"\" Vision Transformer\n    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`  -\n        https://arxiv.org/abs/2010.11929\n    \"\"\"\n\n    def __init__(self, depth=[1, 2, 5, 3], img_size=224, in_chans=3, num_classes=1000, embed_dim=[48, 96, 240, 384],\n                 head_dim=64, mlp_ratio=4., qkv_bias=True, qk_scale=None, representation_size=None,\n                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=None, sr_ratios=[4, 2, 2, 1], **kwargs):\n        \"\"\"\n        Args:\n            depth (list): depth of each stage\n            img_size (int, tuple): input image size\n            in_chans (int): number of input channels\n            num_classes (int): number of classes for classification head\n            embed_dim (list): embedding dimension of each stage\n            head_dim (int): head dimension\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n            qkv_bias (bool): enable bias for qkv if True\n            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n            drop_rate (float): dropout rate\n            attn_drop_rate (float): attention dropout rate\n            drop_path_rate (float): stochastic depth rate\n            norm_layer (nn.Module): normalization layer\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n\n        self.patch_embed1 = PatchEmbed(\n            img_size=img_size, patch_size=4, in_chans=in_chans, embed_dim=embed_dim[0])\n        self.patch_embed2 = PatchEmbed(\n            img_size=img_size // 4, patch_size=2, in_chans=embed_dim[0], embed_dim=embed_dim[1])\n        self.patch_embed3 = PatchEmbed(\n            img_size=img_size // 8, patch_size=2, in_chans=embed_dim[1], embed_dim=embed_dim[2])\n        self.patch_embed4 = PatchEmbed(\n            img_size=img_size // 16, patch_size=2, in_chans=embed_dim[2], embed_dim=embed_dim[3])\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depth))]  # stochastic depth decay rule\n        num_heads = [dim // head_dim for dim in embed_dim]\n        self.blocks1 = nn.ModuleList([\n            LGLBlock(\n                dim=embed_dim[0], num_heads=num_heads[0], mlp_ratio=mlp_ratio[0], qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n                sr_ratio=sr_ratios[0])\n            for i in range(depth[0])])\n        self.blocks2 = nn.ModuleList([\n            LGLBlock(\n                dim=embed_dim[1], num_heads=num_heads[1], mlp_ratio=mlp_ratio[1], qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i + depth[0]], norm_layer=norm_layer,\n                sr_ratio=sr_ratios[1])\n            for i in range(depth[1])])\n        self.blocks3 = nn.ModuleList([\n            LGLBlock(\n                dim=embed_dim[2], num_heads=num_heads[2], mlp_ratio=mlp_ratio[2], qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i + depth[0] + depth[1]], norm_layer=norm_layer,\n                sr_ratio=sr_ratios[2])\n            for i in range(depth[2])])\n        self.blocks4 = nn.ModuleList([\n            LGLBlock(\n                dim=embed_dim[3], num_heads=num_heads[3], mlp_ratio=mlp_ratio[3], qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i + depth[0] + depth[1] + depth[2]],\n                norm_layer=norm_layer, sr_ratio=sr_ratios[3])\n            for i in range(depth[3])])\n        self.norm = nn.BatchNorm2d(embed_dim[-1])\n\n        # Representation layer\n        if representation_size:\n            self.num_features = representation_size\n            self.pre_logits = nn.Sequential(OrderedDict([\n                ('fc', nn.Linear(embed_dim, representation_size)),\n                ('act', nn.Tanh())\n            ]))\n        else:\n            self.pre_logits = nn.Identity()\n\n        # Classifier head\n        self.head = nn.Linear(embed_dim[-1], num_classes) if num_classes > 0 else nn.Identity()\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=''):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.patch_embed1(x)\n        x = self.pos_drop(x)\n        for blk in self.blocks1:\n            x = blk(x)\n        x = self.patch_embed2(x)\n        for blk in self.blocks2:\n            x = blk(x)\n        x = self.patch_embed3(x)\n        for blk in self.blocks3:\n            x = blk(x)\n        x = self.patch_embed4(x)\n        for blk in self.blocks4:\n            x = blk(x)\n        x = self.norm(x)\n        x = self.pre_logits(x)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = x.flatten(2).mean(-1)\n        x = self.head(x)\n        return x\n\n\n@register_model\ndef edgevit_xxs(pretrained=True, **kwargs):\n    model = EdgeVit(\n        depth=[1, 1, 3, 2],\n        embed_dim=[36, 72, 144, 288], head_dim=36, mlp_ratio=[4] * 4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), sr_ratios=[4, 2, 2, 1], **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\n\n@register_model\ndef edgevit_xs(pretrained=True, **kwargs):\n    model = EdgeVit(\n        depth=[1, 1, 3, 1],\n        embed_dim=[48, 96, 240, 384], head_dim=48, mlp_ratio=[4] * 4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), sr_ratios=[4, 2, 2, 1], **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\n\n@register_model\ndef edgevit_s(pretrained=True, **kwargs):\n    model = EdgeVit(\n        depth=[1, 2, 5, 3],\n        embed_dim=[48, 96, 240, 384], head_dim=48, mlp_ratio=[4] * 4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), sr_ratios=[4, 2, 2, 1], **kwargs)\n    model.default_cfg = _cfg()\n    return model\n    \n@register_model\ndef edgevit_m(pretrained=False, **kwargs):\n    model = EdgeVit(\n        depth=[2, 3, 6, 4],  # Increased depth for more layers per stage\n        embed_dim=[64, 128, 320, 512],  # Wider embeddings for higher capacity\n        head_dim=64,  # Increased head dimension for finer attention\n        mlp_ratio=[4, 4, 4.5, 4.5],  # Slightly higher MLP ratio in later stages\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        sr_ratios=[4, 2, 2, 1],  # Kept the same, as these are tied to spatial reduction\n        **kwargs\n    )\n    model.default_cfg = _cfg()\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:05:28.223589Z","iopub.execute_input":"2025-05-01T07:05:28.223895Z","iopub.status.idle":"2025-05-01T07:05:32.256776Z","shell.execute_reply.started":"2025-05-01T07:05:28.223877Z","shell.execute_reply":"2025-05-01T07:05:32.255963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter\n\n\nCK_EMOTION_MAP = {\n    0: \"angry\",     # 0: Anger\n    1: \"disgust\",   # 1: Disgust\n    2: \"fear\",      # 2: Fear\n    3: \"happy\",     # 3: Happiness\n    4: \"sad\",       # 4: Sadness\n    5: \"surprise\",  # 5: Surprise\n    6: \"neutral\"    # 6: Neutral\n   \n}\n\n\nCK_TO_FER_MAP = {\n    0: 2,  # angry: 2\n    1: 1,  # disgust: 1\n    2: 4,  # fear: 4\n    3: 6,  # happy: 6\n    4: 0,  # sad: 0\n    5: 5,  # surprise: 5\n    6: 3,  # neutral: 3\n}\n\nclass CKPlusDataset(Dataset):\n    def __init__(self, data, labels, transform=None):\n        self.data = data\n        self.labels = labels\n        self.transform = transform\n    \n    def frequency_transform(self, image):\n        \"\"\"Convert image into 3-channel format (Original, Low-Pass, High-Pass).\"\"\"\n        if len(image.shape) == 3 and image.shape[-1] == 3:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        \n        image = image.astype(np.float32) / 255.0\n        \n        # Low-Pass Filter (Gaussian Blur)\n        low_pass = cv2.GaussianBlur(image, (7, 7), 1)\n        \n        # High-Pass Filter (Sobel Edge Detection)\n        sobelx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n        sobely = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n        high_pass = cv2.magnitude(sobelx, sobely)\n        high_pass = high_pass / np.max(high_pass) if np.max(high_pass) != 0 else high_pass  # Normalize\n        \n        # Stack into 3-channel image\n        freq_image = np.stack([image, low_pass, high_pass], axis=-1)\n        freq_image = np.transpose(freq_image, (2, 0, 1))  # Convert HWC to CHW\n        \n        return freq_image.astype(np.float32)\n    \n    def __getitem__(self, index):\n        # Rreshape to 48x48\n        pixels = self.data[index]\n        image = pixels.reshape(48, 48)\n        \n     \n        image = self.frequency_transform(image)\n        \n        # Convert (C, H, W) -> (H, W, C) for PIL\n        image = np.transpose(image, (1, 2, 0))\n        \n   \n        image = Image.fromarray((image * 255).astype(np.uint8))\n        \n        # Apply transforms\n        if self.transform:\n            image = self.transform(image)\n        \n        return image, self.labels[index]\n    \n    def __len__(self):\n        return len(self.data)\n\ndef load_ck_plus_dataset(csv_path, test_size=0.2, random_state=42):\n    \"\"\"\n    Load the CK+ dataset from CSV, excluding the contempt class (emotion=7)\n    \"\"\"\n\n    df = pd.read_csv(csv_path)\n    \n    # Filter out contempt class (emotion=7)\n    df = df[df['emotion'] != 7]\n    \n    # Convert pixel strings to numpy arrays\n    pixels = df['pixels'].apply(lambda x: np.array([int(p) for p in x.split()], dtype=np.uint8))\n    pixels = np.stack(pixels.values)\n    \n    # Mapping  to FER indices for consistency\n    emotions = df['emotion'].values\n    mapped_emotions = np.array([CK_TO_FER_MAP[e] for e in emotions])\n    \n    # Split data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        pixels, mapped_emotions, test_size=test_size, random_state=random_state, stratify=mapped_emotions\n    )\n    \n\n    print(\"Original Class Distribution:\", Counter(mapped_emotions))\n    \n   \n    oversampler = RandomOverSampler(random_state=random_state)\n    X_train_reshaped = X_train.reshape(X_train.shape[0], -1)  # Flatten for oversampling\n    X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_reshaped, y_train)\n    X_train_resampled = X_train_resampled.reshape(-1, 48*48)  # Keep as flattened arrays\n    \n    print(\"Train Class Distribution after oversampling:\", Counter(y_train_resampled))\n    print(\"Test Class Distribution:\", Counter(y_test))\n    \n    return X_train_resampled, y_train_resampled, X_test, y_test\n\ndef create_ck_plus_dataloaders(csv_path, batch_size=32, num_workers=4):\n    \"\"\"\n    Create train and test DataLoaders for the CK+ dataset\n    \"\"\"\n    # Load and split the dataset\n    X_train, y_train, X_test, y_test = load_ck_plus_dataset(csv_path)\n    \n    # Transformation Definition\n    train_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomRotation(degrees=20),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5], std=[0.5])\n    ])\n    \n    test_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5], std=[0.5])\n    ])\n    \n  \n    train_dataset = CKPlusDataset(X_train, y_train, transform=train_transform)\n    test_dataset = CKPlusDataset(X_test, y_test, transform=test_transform)\n    \n \n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=num_workers,\n        pin_memory=True\n    )\n    \n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=num_workers,\n        pin_memory=True\n    )\n    \n    return train_loader, test_loader\n\n\n\n\ncsv_path = \"/kaggle/input/ckdataset/ckextended.csv\"\n    \ntrain_loader, test_loader = create_ck_plus_dataloaders(csv_path)\n\nfor images, labels in train_loader:\n        print(f\"Batch shape: {images.shape}, Labels: {labels.shape}\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:05:42.567461Z","iopub.execute_input":"2025-05-01T07:05:42.568231Z","iopub.status.idle":"2025-05-01T07:05:45.357250Z","shell.execute_reply.started":"2025-05-01T07:05:42.568197Z","shell.execute_reply":"2025-05-01T07:05:45.356445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport random\nfrom PIL import Image\nimport cv2\n\n\nimport torchvision.transforms as transforms\n\ndef visualize_sample(dataset, index):\n    \"\"\"\n    Visualize a single sample from the dataset showing the original image\n    and the three channels after frequency transformation\n    \"\"\"\n\n    raw_pixels = dataset.data[index]\n    raw_image = raw_pixels.reshape(48, 48)\n    \n    # Create the frequency transformed image\n    freq_image = dataset.frequency_transform(raw_image)\n    \n    # Get the label\n    label = dataset.labels[index]\n    fer_label = label  # This is already mapped to FER index\n    \n    # Get the emotion name\n    emotion_name = None\n    for ck_idx, fer_idx in CK_TO_FER_MAP.items():\n        if fer_idx == fer_label:\n            emotion_name = CK_EMOTION_MAP[ck_idx]\n            break\n    \n   \n    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n    \n   \n    axes[0].imshow(raw_image, cmap='gray')\n    axes[0].set_title(f\"Original Image\\nEmotion: {emotion_name} (FER idx: {fer_label})\")\n    axes[0].axis('off')\n    \n  \n    channel_names = [\"Original\", \"Low-Pass\", \"High-Pass\"]\n    for i in range(3):\n        axes[i+1].imshow(freq_image[i], cmap='gray')\n        axes[i+1].set_title(f\"Channel {i}: {channel_names[i]}\")\n        axes[i+1].axis('off')\n    \n    plt.tight_layout()\n    return fig\n\ndef visualize_random_samples(csv_path, num_samples=3):\n    \"\"\"\n    Visualize random samples from the CK+ dataset after preprocessing\n    \"\"\"\n\n    X_train, y_train, X_test, y_test = load_ck_plus_dataset(csv_path)\n    \n    train_dataset = CKPlusDataset(X_train, y_train, transform=None)\n    \n    indices = random.sample(range(len(train_dataset)), num_samples)\n    \n    figures = []\n    for idx in indices:\n        fig = visualize_sample(train_dataset, idx)\n        figures.append(fig)\n    \n    plt.show()\n    \n    return figures\n\ndef visualize_batch_from_loader(train_loader):\n    \"\"\"\n    Visualize a batch from the data loader to check tensor shapes and transforms\n    \"\"\"\n   \n    images, labels = next(iter(train_loader))\n    \n\n    print(f\"Batch shape: {images.shape}\")\n    print(f\"Labels shape: {labels.shape}\")\n    \n   \n    for i in range(min(3, len(images))):\n        # Convert tensor to numpy array (C, H, W) -> (H, W, C)\n        img = images[i].numpy().transpose(1, 2, 0)\n        \n        # Denormalize the image: img = (img * std) + mean\n        img = (img * 0.5) + 0.5\n        \n        # Clipping  values to [0, 1] range\n        img = np.clip(img, 0, 1)\n        \n        label = labels[i].item()\n        \n        emotion_name = None\n        for ck_idx, fer_idx in CK_TO_FER_MAP.items():\n            if fer_idx == label:\n                emotion_name = CK_EMOTION_MAP[ck_idx]\n                break\n        \n        # Create figure\n        plt.figure(figsize=(15, 5))\n        \n        # Display each channel separately\n        for c in range(3):\n            plt.subplot(1, 3, c+1)\n            plt.imshow(img[:, :, c], cmap='gray')\n            plt.title(f\"Channel {c}\")\n            plt.axis('off')\n        \n        plt.suptitle(f\"Emotion: {emotion_name} (FER idx: {label})\")\n        plt.tight_layout()\n        plt.show()\n\n\n\ncsv_path = \"/kaggle/input/ckdataset/ckextended.csv\"  \n\n\ntrain_loader, test_loader = create_ck_plus_dataloaders(csv_path, batch_size=16)\n\nprint(\"Visualizing random samples from the dataset...\")\nvisualize_random_samples(csv_path, num_samples=3)\n\n\n# print(\"\\nVisualizing samples from the data loader...\")\n# visualize_batch_from_loader(train_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:05:46.777633Z","iopub.execute_input":"2025-05-01T07:05:46.777949Z","iopub.status.idle":"2025-05-01T07:05:50.330780Z","shell.execute_reply.started":"2025-05-01T07:05:46.777905Z","shell.execute_reply":"2025-05-01T07:05:50.330006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport time\nimport copy\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nfrom tqdm import tqdm\n\n\n\nimport torch\nimport torch.nn as nn\n\ndef load_pretrained_model_from_url(url, num_classes=7):\n    \"\"\"\n    Load a pre-trained EdgeViT_XXS model from a direct URL and replace the classifier head.\n    \n    Args:\n        url (str): Direct URL to the .pth file.\n        num_classes (int): Number of output classes.\n    \"\"\"\n\n    model = edgevit_xxs(num_classes=num_classes) \n    \n\n    try:\n        checkpoint = torch.hub.load_state_dict_from_url(url, map_location='cpu', progress=True)\n        \n   \n        if 'state_dict' in checkpoint:\n            checkpoint = checkpoint['state_dict']\n        elif 'model_state_dict' in checkpoint:\n            checkpoint = checkpoint['model_state_dict']\n        \n        model.load_state_dict(checkpoint, strict=False)\n        print(\"Successfully loaded model weights from URL\")\n        \n        return model\n    \n    except Exception as e:\n        print(f\"Error loading model from URL: {e}\")\n        raise\n\n\ndef train_model(model, dataloaders, criterion, optimizer, scheduler, device, num_epochs=25):\n    \"\"\"\n    Train the model on the CK+ dataset\n    \"\"\"\n    since = time.time()\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    train_losses = []\n    val_losses = []\n    train_accs = []\n    val_accs = []\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print('-' * 10)\n        \n       \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  \n            else:\n                model.eval()  \n            \n            running_loss = 0.0\n            running_corrects = 0\n            \n\n            for inputs, labels in tqdm(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                # Zero the parameter gradients\n                optimizer.zero_grad()\n                \n                # Forward pass\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    \n                    # Backward + optimize\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                \n                # Statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            \n            if phase == 'train':\n                scheduler.step()\n            \n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n            \n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n            \n            # Record metrics\n            if phase == 'train':\n                train_losses.append(epoch_loss)\n                train_accs.append(epoch_acc.item())\n            else:\n                val_losses.append(epoch_loss)\n                val_accs.append(epoch_acc.item())\n            \n            # SAve model if it's the best validation accuracy\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n        \n        print()\n    \n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:.4f}')\n    \n\n    model.load_state_dict(best_model_wts)\n    \n\n    plot_training_curves(train_losses, val_losses, train_accs, val_accs)\n    \n    return model\n\ndef plot_training_curves(train_losses, val_losses, train_accs, val_accs):\n    \"\"\"\n    Plot training and validation curves\n    \"\"\"\n    plt.figure(figsize=(12, 5))\n    \n    # Loss plot\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label='Training Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n    \n    # Accuracy plot\n    plt.subplot(1, 2, 2)\n    plt.plot(train_accs, label='Training Accuracy')\n    plt.plot(val_accs, label='Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig('training_curves.png')\n    plt.show()\n\ndef evaluate_model(model, test_loader, device):\n    \"\"\"\n    Evaluate the trained model on the test set\n    \"\"\"\n    model.eval()\n    \n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Print classification report\n    print(\"\\nClassification Report:\")\n    class_names = [CK_EMOTION_MAP[i] for i in range(7)]\n    report = classification_report(all_labels, all_preds, target_names=class_names)\n    print(report)\n    \n    # Plot confusion matrix\n    plot_confusion_matrix(all_labels, all_preds, class_names)\n    \n    return all_preds, all_labels\n\ndef plot_confusion_matrix(y_true, y_pred, class_names):\n    \"\"\"\n    Plot the confusion matrix\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.savefig('confusion_matrix.png')\n    plt.show()\n\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ncsv_path = \"/kaggle/input/ckdataset/ckextended.csv\" \n\n\nrepo_id = \"abhishek/edgevit_xxs_fer\" \nmodel_name = \"edgevit_xs\"  \n\n\ndataloaders = {\n    'train': train_loader,\n    'val': test_loader\n}\n\nmodel_url = \"https://huggingface.co/abhishek/edgevit_xxs_fer/resolve/main/edgevit_distilled.pth\"\nmodel = load_pretrained_model_from_url(model_url, num_classes=7)\n\n\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n\nscheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\n\nmodel = train_model(model, dataloaders, criterion, optimizer, scheduler, device, num_epochs=25)\n\n\n# torch.save({\n#     'epoch': 25,\n#     'model_state_dict': model.state_dict(),\n#     'optimizer_state_dict': optimizer.state_dict(),\n# }, 'edgevit_xxs_ckplus_finetuned.pth')\ntorch.save(model.state_dict(), \"edgevit_finetuned.pth\")\n\n# Evaluate the model\nevaluate_model(model, test_loader, device)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:05:54.001646Z","iopub.execute_input":"2025-05-01T07:05:54.002377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
